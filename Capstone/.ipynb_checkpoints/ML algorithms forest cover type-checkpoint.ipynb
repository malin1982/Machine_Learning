{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<....Work in progress...>\n",
    "\n",
    "Thank you for opening this script!\n",
    "\n",
    "I have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.\n",
    "\n",
    "Please **upvote** this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks.\n",
    "***\n",
    "## Layout of the document\n",
    "The prediction process is divided into two notebooks.\n",
    "\n",
    "Part 1 : Covers data statistics, data visualization, and feature selection : https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-on-feature-selection\n",
    "\n",
    "This notebook : Covers prediction using various algorithms \n",
    "***\n",
    "## Data statistics\n",
    "* Shape\n",
    "* Datatypes\n",
    "* Description\n",
    "* Skew\n",
    "* Class distribution\n",
    "\n",
    "## Data Interaction\n",
    "* Correlation\n",
    "* Scatter plot\n",
    "\n",
    "## Data Visualization\n",
    "* Box and density plots\n",
    "* Grouping of one hot encoded attributes\n",
    "\n",
    "## Data Cleaning\n",
    "* Remove unnecessary columns\n",
    "\n",
    "## Data Preparation\n",
    "* Original\n",
    "* Delete rows or impute values in case of missing\n",
    "* StandardScaler\n",
    "* MinMaxScaler\n",
    "* Normalizer\n",
    "\n",
    "## Feature selection\n",
    "* ExtraTreesClassifier\n",
    "* GradientBoostingClassifier\n",
    "* RandomForestClassifier\n",
    "* XGBClassifier\n",
    "* RFE\n",
    "* SelectPercentile\n",
    "* PCA\n",
    "* PCA + SelectPercentile\n",
    "* Feature Engineering\n",
    "\n",
    "## Evaluation, prediction, and analysis\n",
    "* LDA (Linear algo)\n",
    "* LR (Linear algo)\n",
    "* KNN (Non-linear algo)\n",
    "* CART (Non-linear algo)\n",
    "* Naive Bayes (Non-linear algo)\n",
    "* SVC (Non-linear algo)\n",
    "* Bagged Decision Trees (Bagging)\n",
    "* Random Forest (Bagging)\n",
    "* Extra Trees (Bagging)\n",
    "* AdaBoost (Boosting)\n",
    "* Stochastic Gradient Boosting (Boosting)\n",
    "* Voting Classifier (Voting)\n",
    "* MLP (Deep Learning)\n",
    "* XGBoost\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data:\n",
    "\n",
    "Information about all the attributes can be found here:\n",
    "\n",
    "https://www.kaggle.com/c/forest-cover-type-prediction/data\n",
    "\n",
    "Learning: \n",
    "We need to predict the 'Cover_Type' based on the other attributes. Hence, this is a classification problem where the target could belong to any of the seven classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supress unnecessary warnings so that presentation looks clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read raw data from the file\n",
    "\n",
    "import pandas #provides data structures to quickly analyze data\n",
    "#Since this code runs on Kaggle server, train data can be accessed directly in the 'input' folder\n",
    "dataset = pandas.read_csv(\"../input/train.csv\") \n",
    "\n",
    "#Drop the first column 'Id' since it just has serial numbers. Not useful in the prediction process.\n",
    "dataset = dataset.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "* Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removal list initialize\n",
    "rem = []\n",
    "\n",
    "#Add constant columns as they don't help in prediction process\n",
    "for c in dataset.columns:\n",
    "    if dataset[c].std() == 0: #standard deviation is zero\n",
    "        rem.append(c)\n",
    "\n",
    "#drop the columns        \n",
    "dataset.drop(rem,axis=1,inplace=True)\n",
    "\n",
    "print(rem)\n",
    "\n",
    "#Following columns are dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "* Original\n",
    "* Delete rows or impute values in case of missing\n",
    "* StandardScaler\n",
    "* MinMaxScaler\n",
    "* Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the number of rows and columns\n",
    "r, c = dataset.shape\n",
    "\n",
    "#get the list of columns\n",
    "cols = dataset.columns\n",
    "#create an array which has indexes of columns\n",
    "i_cols = []\n",
    "for i in range(0,c-1):\n",
    "    i_cols.append(i)\n",
    "#array of importance rank of all features  \n",
    "ranks = []\n",
    "\n",
    "#Extract only the values\n",
    "array = dataset.values\n",
    "\n",
    "#Y is the target column, X has the rest\n",
    "X_orig = array[:,0:(c-1)]\n",
    "Y = array[:,(c-1)]\n",
    "\n",
    "#Validation chunk size\n",
    "val_size = 0.1\n",
    "\n",
    "#Use a common seed in all experiments so that same chunk is used for validation\n",
    "seed = 0\n",
    "\n",
    "#Split the data into chunks\n",
    "from sklearn import cross_validation\n",
    "X_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X_orig, Y, test_size=val_size, random_state=seed)\n",
    "\n",
    "#Import libraries for data transformations\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#All features\n",
    "X_all = []\n",
    "#Additionally we will make a list of subsets\n",
    "X_all_add =[]\n",
    "\n",
    "#columns to be dropped\n",
    "rem_cols = []\n",
    "#indexes of columns to be dropped\n",
    "i_rem = []\n",
    "\n",
    "#Add this version of X to the list \n",
    "X_all.append(['Orig','All', X_train,X_val,1.0,cols[:c-1],rem_cols,ranks,i_cols,i_rem])\n",
    "\n",
    "#point where categorical data begins\n",
    "size=10\n",
    "\n",
    "import numpy\n",
    "\n",
    "#Standardized\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = StandardScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['StdSca','All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n",
    "\n",
    "#MinMax\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['MinMax', 'All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n",
    "\n",
    "#Normalize\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = Normalizer().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['Norm', 'All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n",
    "\n",
    "#Impute\n",
    "#Imputer is not used as no data is missing\n",
    "\n",
    "#List of transformations\n",
    "trans_list = []\n",
    "\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "    trans_list.append(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Using the rankings produced in :\n",
    "https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-on-feature-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select top 75%,50%,25%\n",
    "ratio_list = [0.75,0.50,0.25]\n",
    "\n",
    "#Median of rankings for each column\n",
    "unsorted_rank = [0,8,11,4,5,2,5,7.5,9.5,3,8,28.5,14.5,2,35,19.5,12,14,37,25.5,50,44,9,28,20.5,19.5,40,38,20,38,43,35,44,22,24,33,49,42,46,47,27.5,19,31.5,23,28,42,30.5,46,40,12,13,18]\n",
    "\n",
    "#List of feature selection models\n",
    "feat = []\n",
    "\n",
    "#Add Median to the list \n",
    "n = 'Median'\n",
    "for val in ratio_list:\n",
    "    feat.append([n,val])   \n",
    "\n",
    "for trans,s, X, X_val, d, cols, rem_cols, ra, i_cols, i_rem in X_all:\n",
    "    #Create subsets of feature list based on ranking and ratio_list\n",
    "    for name, v in feat:\n",
    "        #Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        for i, pred in enumerate(unsorted_rank):\n",
    "            joined.append([i,cols[i],pred])\n",
    "        #Sort in descending order    \n",
    "        joined_sorted = sorted(joined, key=lambda x: x[2])\n",
    "        #Starting point of the columns to be dropped\n",
    "        rem_start = int((v*(c-1)))\n",
    "        #List of names of columns selected\n",
    "        cols_list = []\n",
    "        #Indexes of columns selected\n",
    "        i_cols_list = []\n",
    "        #Ranking of all the columns\n",
    "        rank_list =[]\n",
    "        #List of columns not selected\n",
    "        rem_list = []\n",
    "        #Indexes of columns not selected\n",
    "        i_rem_list = []\n",
    "        #Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            #Store the rank\n",
    "            rank_list.append([i,j])\n",
    "            #Store selected columns in cols_list and indexes in i_cols_list\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            #Store not selected columns in rem_list and indexes in i_rem_list    \n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)    \n",
    "        #Sort the rank_list and store only the ranks. Drop the index \n",
    "        #Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import plotting library    \n",
    "import matplotlib.pyplot as plt    \n",
    "\n",
    "#Dictionary to store the accuracies for all combinations \n",
    "acc = {}\n",
    "\n",
    "#List of combinations\n",
    "comb = []\n",
    "\n",
    "#Append name of transformation to trans_list\n",
    "for trans in trans_list:\n",
    "    acc[trans]=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* LDA (Linear algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of LinearDiscriminatAnalysis using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#Set the base model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "algo = \"LDA\"\n",
    "\n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Accuracy of the model using all features\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "    model.fit(X[:,i_cols_list],Y_train)\n",
    "    result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "    acc[trans].append(result)\n",
    "    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #print(result)\n",
    "comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n",
    "        \n",
    "#Accuracy of the model using a subset of features    \n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "    model.fit(X[:,i_cols_list],Y_train)\n",
    "    result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "    acc[trans].append(result)\n",
    "    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #print(result)\n",
    "for v in ratio_list:\n",
    "    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is 65%. Occurs when all features are used and without any transformation!\n",
    "#Performance of MinMax and Normalizer is very poor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* LR (Linear algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of LogisticRegression using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "C_list = [100]\n",
    "\n",
    "for C in C_list:\n",
    "    #Set the base model\n",
    "    model = LogisticRegression(n_jobs=-1,random_state=seed,C=C)\n",
    "   \n",
    "    algo = \"LR\"\n",
    "\n",
    "    ##Set figure size\n",
    "    #plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"All\",1.0))\n",
    "\n",
    "    #Accuracy of the model using a subset of features    \n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    for v in ratio_list:\n",
    "        comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "      \n",
    "#Best estimated performance is close to 67% with LR when C=100 and all attributes are considered and with standardized data\n",
    "#Performance improves will increasing value of C\n",
    "#Performance of Normalizer and MinMax Scaler is poor in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* KNN (Non-linear algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of KNN Classifier using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_list = [1]\n",
    "\n",
    "for n_neighbors in n_list:\n",
    "    #Set the base model\n",
    "    model = KNeighborsClassifier(n_jobs=-1,n_neighbors=n_neighbors)\n",
    "   \n",
    "    algo = \"KNN\"\n",
    "\n",
    "    ##Set figure size\n",
    "    #plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_neighbors,\"All\",1.0))\n",
    "\n",
    "    #Accuracy of the model using a subset of features    \n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    for v in ratio_list:\n",
    "        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_neighbors,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    " \n",
    "#Best estimated performance is close to 86% when n_neighbors=1 and normalizer is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of Naive Bayes using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Set the base model\n",
    "model = GaussianNB()\n",
    "algo = \"NB\"\n",
    "\n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Accuracy of the model using all features\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "    model.fit(X[:,i_cols_list],Y_train)\n",
    "    result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "    acc[trans].append(result)\n",
    "    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #print(result)\n",
    "comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n",
    "        \n",
    "#Accuracy of the model using a subset of features    \n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "    model.fit(X[:,i_cols_list],Y_train)\n",
    "    result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "    acc[trans].append(result)\n",
    "    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #print(result)\n",
    "for v in ratio_list:\n",
    "    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is close to 64%. Original with 50% subset outperfoms all transformations of NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* CART (Non-linear algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of CART using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "d_list = [13]\n",
    "\n",
    "for max_depth in d_list:\n",
    "    #Set the base model\n",
    "    model = DecisionTreeClassifier(random_state=seed,max_depth=max_depth)\n",
    "   \n",
    "    algo = \"CART\"\n",
    "\n",
    "    #Set figure size\n",
    "    plt.rc(\"figure\", figsize=(15, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"All\",1.0))\n",
    "\n",
    "    #Accuracy of the model using a subset of features    \n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    for v in ratio_list:\n",
    "        comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "    \n",
    "#Best estimated performance is close to 79% when max_depth=13 and for Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* SVM (Non-linear algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of SVM using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "c_list = [10]\n",
    "\n",
    "for C in c_list:\n",
    "    #Set the base model\n",
    "    model = SVC(random_state=seed,C=C)\n",
    "\n",
    "    algo = \"SVM\"\n",
    "\n",
    "    #Set figure size\n",
    "    #plt.rc(\"figure\", figsize=(15, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"All\",1.0))\n",
    "\n",
    "    ##Accuracy of the model using a subset of features    \n",
    "    #for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "    #    model.fit(X[:,i_cols_list],Y_train)\n",
    "    #    result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "    #    acc[trans].append(result)\n",
    "    #    print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #    print(result)\n",
    "    #for v in ratio_list:\n",
    "    #    comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Training time is very high compared to other algos\n",
    "#Performance is very poor for original. Shows the importance of data transformation\n",
    "#Best estimated performance is close to 77% when C=10 and for StandardScaler with 0.25 subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* Bagged Decision Trees (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of Bagged Decision Trees using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Base estimator\n",
    "base_estimator = DecisionTreeClassifier(random_state=seed,max_depth=13)\n",
    "\n",
    "n_list = [100]\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = BaggingClassifier(n_jobs=-1,base_estimator=base_estimator, n_estimators=n_estimators, random_state=seed)\n",
    "   \n",
    "    algo = \"Bag\"\n",
    "\n",
    "    #Set figure size\n",
    "    plt.rc(\"figure\", figsize=(20, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n",
    "\n",
    "    #Accuracy of the model using a subset of features    \n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    for v in ratio_list:\n",
    "        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is close to 82% when n_estimators is 100 for Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* Random Forest (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of Random Forest using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_list = [100]\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = RandomForestClassifier(n_jobs=-1,n_estimators=n_estimators, random_state=seed)\n",
    "   \n",
    "    algo = \"RF\"\n",
    "\n",
    "    #Set figure size\n",
    "    plt.rc(\"figure\", figsize=(20, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n",
    "\n",
    "    #Accuracy of the model using a subset of features    \n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    for v in ratio_list:\n",
    "        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is close to 85% when n_estimators is 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* Extra Trees (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of Extra Trees using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "n_list = [100]\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = ExtraTreesClassifier(n_jobs=-1,n_estimators=n_estimators, random_state=seed)\n",
    "   \n",
    "    algo = \"ET\"\n",
    "\n",
    "    #Set figure size\n",
    "    plt.rc(\"figure\", figsize=(20, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n",
    "\n",
    "    #Accuracy of the model using a subset of features    \n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    for v in ratio_list:\n",
    "        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is close to 88% when n_estimators is 100 , StdScaler with 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* AdaBoost (Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of AdaBoost ensemble using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "n_list = [100]\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = AdaBoostClassifier(n_estimators=n_estimators, random_state=seed)\n",
    "   \n",
    "    algo = \"Ada\"\n",
    "\n",
    "    #Set figure size\n",
    "    plt.rc(\"figure\", figsize=(20, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n",
    "\n",
    "    #Accuracy of the model using a subset of features    \n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    for v in ratio_list:\n",
    "        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is close to 38% when n_estimators is 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* Gradient Boosting (Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of Stochastic Gradient Boosting using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "d_list = [9]\n",
    "\n",
    "for max_depth in d_list:\n",
    "    #Set the base model\n",
    "    model = GradientBoostingClassifier(max_depth=max_depth, random_state=seed)\n",
    "   \n",
    "    algo = \"SGB\"\n",
    "\n",
    "    #Set figure size\n",
    "    plt.rc(\"figure\", figsize=(20, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"All\",1.0))\n",
    "\n",
    "    ##Accuracy of the model using a subset of features    \n",
    "    #for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "    #    model.fit(X[:,i_cols_list],Y_train)\n",
    "    #    result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "    #    acc[trans].append(result)\n",
    "    #    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #    #print(result)\n",
    "    #for v in ratio_list:\n",
    "    #    comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#training time is too high\n",
    "#Best estimated performance is close to 86% when depth is 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* Voting Classifier (Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of Voting Classifier using all the views\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "list_estimators =[]\n",
    "\n",
    "estimators = []\n",
    "model1 = ExtraTreesClassifier(n_jobs=-1,n_estimators=100, random_state=seed)\n",
    "estimators.append(('et', model1))\n",
    "model2 = RandomForestClassifier(n_jobs=-1,n_estimators=100, random_state=seed)\n",
    "estimators.append(('rf', model2))\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "base_estimator = DecisionTreeClassifier(random_state=seed,max_depth=13)\n",
    "model3 = BaggingClassifier(n_jobs=-1,base_estimator=base_estimator, n_estimators=100, random_state=seed)\n",
    "estimators.append(('bag', model3))\n",
    "\n",
    "list_estimators.append(['Voting',estimators])\n",
    "\n",
    "for name, estimators in list_estimators:\n",
    "    #Set the base model\n",
    "    model = VotingClassifier(estimators=estimators, n_jobs=-1)\n",
    "   \n",
    "    algo = name\n",
    "\n",
    "    #Set figure size\n",
    "    plt.rc(\"figure\", figsize=(20, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n",
    "\n",
    "    #Accuracy of the model using a subset of features    \n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    for v in ratio_list:\n",
    "        comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is close to 86%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of various combinations of XG Boost using all the views\n",
    "\n",
    "#Import the library\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "n_list = [300]\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = XGBClassifier(n_estimators=n_estimators, seed=seed,subsample=0.25)\n",
    "   \n",
    "    algo = \"XGB\"\n",
    "\n",
    "    #Set figure size\n",
    "    plt.rc(\"figure\", figsize=(20, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n",
    "\n",
    "    #Accuracy of the model using a subset of features    \n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        model.fit(X[:,i_cols_list],Y_train)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "        acc[trans].append(result)\n",
    "        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "        #print(result)\n",
    "    for v in ratio_list:\n",
    "        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is close to 80% when n_estimators is 300, sub_sample=0.25 , subset=0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, prediction, and analysis\n",
    "* Multi-layer perceptrons (Deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation of baseline model of MLP using all the views\n",
    "\n",
    "#Import libraries for deep learning\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Import libraries for encoding\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#no. of output classes\n",
    "y = 7\n",
    "\n",
    "#random state\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# one hot encode class values\n",
    "encoder = LabelEncoder()\n",
    "Y_train_en = encoder.fit_transform(Y_train)\n",
    "Y_train_hot = np_utils.to_categorical(Y_train_en,y) \n",
    "Y_val_en = encoder.fit_transform(Y_val)\n",
    "Y_val_hot = np_utils.to_categorical(Y_val_en,y) \n",
    "\n",
    "\n",
    "# define baseline model\n",
    "def baseline(v):\n",
    "     # create model\n",
    "     model = Sequential()\n",
    "     model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n",
    "     model.add(Dense(y, init='normal', activation='sigmoid'))\n",
    "     # Compile model\n",
    "     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "     return model\n",
    "\n",
    "# define smaller model\n",
    "def smaller(v):\n",
    " # create model\n",
    " model = Sequential()\n",
    " model.add(Dense(v*(c-1)/2, input_dim=v*(c-1), init='normal', activation='relu'))\n",
    " model.add(Dense(y, init='normal', activation='sigmoid'))\n",
    " # Compile model\n",
    " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "# define deeper model\n",
    "def deeper(v):\n",
    " # create model\n",
    " model = Sequential()\n",
    " model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n",
    " model.add(Dense(v*(c-1)/2, init='normal', activation='relu'))\n",
    " model.add(Dense(y, init='normal', activation='sigmoid'))\n",
    " # Compile model\n",
    " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "# Optimize using dropout and decay\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def dropout(v):\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu',W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(v*(c-1)/2, init='normal', activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y, init='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define decay model\n",
    "def decay(v):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n",
    "    model.add(Dense(y, init='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.1,momentum=0.8,decay=0.01,nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "est_list = [('MLP',baseline),('smaller',smaller),('deeper',deeper),('dropout',dropout),('decay',decay)]\n",
    "\n",
    "for name, est in est_list:\n",
    " \n",
    "    algo = name\n",
    "\n",
    "    #Set figure size\n",
    "    plt.rc(\"figure\", figsize=(20, 10))\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        model = KerasClassifier(build_fn=est, v=v, nb_epoch=10, verbose=0)\n",
    "        model.fit(X[:,i_cols_list],Y_train_hot)\n",
    "        result = model.score(X_val[:,i_cols_list], Y_val_hot)\n",
    "        acc[trans].append(result)\n",
    "    #    print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #    print(result)\n",
    "    comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n",
    "\n",
    "    ##Accuracy of the model using a subset of features    \n",
    "    #for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "    #    model = KerasClassifier(build_fn=est, v=v, nb_epoch=10, verbose=0)\n",
    "    #    model.fit(X[:,i_cols_list],Y_train_hot)\n",
    "    #    result = model.score(X_val[:,i_cols_list], Y_val_hot)\n",
    "    #    acc[trans].append(result)\n",
    "    #    print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #    print(result)\n",
    "    #for v in ratio_list:\n",
    "    #    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n",
    "\n",
    "#Plot the accuracies of all combinations\n",
    "fig, ax = plt.subplots()\n",
    "#Plot each transformation\n",
    "for trans in trans_list:\n",
    "        plt.plot(acc[trans])\n",
    "#Set the tick names to names of combinations\n",
    "ax.set_xticks(range(len(comb)))\n",
    "ax.set_xticklabels(comb,rotation='vertical')\n",
    "#Display the plot\n",
    "plt.legend(trans_list,loc='best')    \n",
    "#Plot the accuracy for all combinations\n",
    "plt.show()    \n",
    "\n",
    "# Best estimated performance is 71% \n",
    "# Performance is poor is general. Data transformations make a huge difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions using Extra Tress Classifier + 0.5 subset as it gave the best estimated performance\n",
    "\n",
    "n_estimators = 100\n",
    "\n",
    "#Obtain the list of indexes for the required model\n",
    "indexes = []\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "    if v == 0.5:\n",
    "        if trans == 'Orig':\n",
    "            indexes = i_cols_list\n",
    "            break\n",
    "\n",
    "#Best model definition\n",
    "best_model = ExtraTreesClassifier(n_jobs=-1,n_estimators=n_estimators)\n",
    "best_model.fit(X_orig[:,indexes],Y)\n",
    "\n",
    "#Read test dataset\n",
    "dataset_test = pandas.read_csv(\"../input/test.csv\")\n",
    "#Drop unnecessary columns\n",
    "ID = dataset_test['Id']\n",
    "dataset_test.drop('Id',axis=1,inplace=True)\n",
    "dataset_test.drop(rem,axis=1,inplace=True)\n",
    "X_test = dataset_test.values\n",
    "\n",
    "#Make predictions using the best model\n",
    "predictions = best_model.predict(X_test[:,indexes])\n",
    "# Write submissions to output file in the correct format\n",
    "with open(\"submission.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(predictions)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
