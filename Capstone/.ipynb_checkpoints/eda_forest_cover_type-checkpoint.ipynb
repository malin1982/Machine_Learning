{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for opening this script!\n",
    "\n",
    "I have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.\n",
    "\n",
    "Please **upvote** this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks.\n",
    "\n",
    "My other exploratory studies can be accessed here :\n",
    "https://www.kaggle.com/sharmasanthosh/kernels\n",
    "***\n",
    "## Layout of the document\n",
    "The prediction process is divided into two notebooks.\n",
    "\n",
    "This notebook : Covers data statistics, data visualization, and feature selection\n",
    "\n",
    "Part 2 : Covers prediction using various algorithms : https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-of-ml-algorithms\n",
    "***\n",
    "## Data statistics\n",
    "* Shape\n",
    "* Datatypes\n",
    "* Description\n",
    "* Skew\n",
    "* Class distribution\n",
    "\n",
    "## Data Interaction\n",
    "* Correlation\n",
    "* Scatter plot\n",
    "\n",
    "## Data Visualization\n",
    "* Box and density plots\n",
    "* Grouping of one hot encoded attributes\n",
    "\n",
    "## Data Cleaning\n",
    "* Remove unnecessary columns\n",
    "\n",
    "## Data Preparation\n",
    "* Original\n",
    "* Delete rows or impute values in case of missing\n",
    "* StandardScaler\n",
    "* MinMaxScaler\n",
    "* Normalizer\n",
    "\n",
    "## Feature selection\n",
    "* ExtraTreesClassifier\n",
    "* GradientBoostingClassifier\n",
    "* RandomForestClassifier\n",
    "* XGBClassifier\n",
    "* RFE\n",
    "* SelectPercentile\n",
    "* PCA\n",
    "* PCA + SelectPercentile\n",
    "* Feature Engineering\n",
    "\n",
    "## Evaluation, prediction, and analysis\n",
    "* LDA (Linear algo)\n",
    "* LR (Linear algo)\n",
    "* KNN (Non-linear algo)\n",
    "* CART (Non-linear algo)\n",
    "* Naive Bayes (Non-linear algo)\n",
    "* SVC (Non-linear algo)\n",
    "* Bagged Decision Trees (Bagging)\n",
    "* Random Forest (Bagging)\n",
    "* Extra Trees (Bagging)\n",
    "* AdaBoost (Boosting)\n",
    "* Stochastic Gradient Boosting (Boosting)\n",
    "* Voting Classifier (Voting)\n",
    "* MLP (Deep Learning)\n",
    "* XGBoost\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data:\n",
    "\n",
    "Information about all the attributes can be found here:\n",
    "\n",
    "https://www.kaggle.com/c/forest-cover-type-prediction/data\n",
    "\n",
    "Learning: \n",
    "We need to predict the 'Cover_Type' based on the other attributes. Hence, this is a classification problem where the target could belong to any of the seven classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supress unnecessary warnings so that presentation looks clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read raw data from the file\n",
    "\n",
    "import pandas #provides data structures to quickly analyze data\n",
    "#Since this code runs on Kaggle server, train data can be accessed directly in the 'input' folder\n",
    "dataset = pandas.read_csv(\"train.csv\") \n",
    "\n",
    "#Drop the first column 'Id' since it just has serial numbers. Not useful in the prediction process.\n",
    "dataset = dataset.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data statistics\n",
    "* Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120, 55)\n"
     ]
    }
   ],
   "source": [
    "# Size of the dataframe\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "# We can see that there are 15120 instances having 55 attributes\n",
    "\n",
    "#Learning : Data is loaded successfully as dimensions match the data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data statistics\n",
    "* Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevation                             int64\n",
      "Aspect                                int64\n",
      "Slope                                 int64\n",
      "Horizontal_Distance_To_Hydrology      int64\n",
      "Vertical_Distance_To_Hydrology        int64\n",
      "Horizontal_Distance_To_Roadways       int64\n",
      "Hillshade_9am                         int64\n",
      "Hillshade_Noon                        int64\n",
      "Hillshade_3pm                         int64\n",
      "Horizontal_Distance_To_Fire_Points    int64\n",
      "Wilderness_Area1                      int64\n",
      "Wilderness_Area2                      int64\n",
      "Wilderness_Area3                      int64\n",
      "Wilderness_Area4                      int64\n",
      "Soil_Type1                            int64\n",
      "Soil_Type2                            int64\n",
      "Soil_Type3                            int64\n",
      "Soil_Type4                            int64\n",
      "Soil_Type5                            int64\n",
      "Soil_Type6                            int64\n",
      "Soil_Type7                            int64\n",
      "Soil_Type8                            int64\n",
      "Soil_Type9                            int64\n",
      "Soil_Type10                           int64\n",
      "Soil_Type11                           int64\n",
      "Soil_Type12                           int64\n",
      "Soil_Type13                           int64\n",
      "Soil_Type14                           int64\n",
      "Soil_Type15                           int64\n",
      "Soil_Type16                           int64\n",
      "Soil_Type17                           int64\n",
      "Soil_Type18                           int64\n",
      "Soil_Type19                           int64\n",
      "Soil_Type20                           int64\n",
      "Soil_Type21                           int64\n",
      "Soil_Type22                           int64\n",
      "Soil_Type23                           int64\n",
      "Soil_Type24                           int64\n",
      "Soil_Type25                           int64\n",
      "Soil_Type26                           int64\n",
      "Soil_Type27                           int64\n",
      "Soil_Type28                           int64\n",
      "Soil_Type29                           int64\n",
      "Soil_Type30                           int64\n",
      "Soil_Type31                           int64\n",
      "Soil_Type32                           int64\n",
      "Soil_Type33                           int64\n",
      "Soil_Type34                           int64\n",
      "Soil_Type35                           int64\n",
      "Soil_Type36                           int64\n",
      "Soil_Type37                           int64\n",
      "Soil_Type38                           int64\n",
      "Soil_Type39                           int64\n",
      "Soil_Type40                           int64\n",
      "Cover_Type                            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Datatypes of the attributes\n",
    "\n",
    "print(dataset.dtypes)\n",
    "\n",
    "# Learning : Data types of all attributes has been inferred as int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data statistics\n",
    "* Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Elevation        Aspect         Slope  \\\n",
      "count  15120.000000  15120.000000  15120.000000   \n",
      "mean    2749.322553    156.676653     16.501587   \n",
      "std      417.678187    110.085801      8.453927   \n",
      "min     1863.000000      0.000000      0.000000   \n",
      "25%     2376.000000     65.000000     10.000000   \n",
      "50%     2752.000000    126.000000     15.000000   \n",
      "75%     3104.000000    261.000000     22.000000   \n",
      "max     3849.000000    360.000000     52.000000   \n",
      "\n",
      "       Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  \\\n",
      "count                      15120.000000                    15120.000000   \n",
      "mean                         227.195701                       51.076521   \n",
      "std                          210.075296                       61.239406   \n",
      "min                            0.000000                     -146.000000   \n",
      "25%                           67.000000                        5.000000   \n",
      "50%                          180.000000                       32.000000   \n",
      "75%                          330.000000                       79.000000   \n",
      "max                         1343.000000                      554.000000   \n",
      "\n",
      "       Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  \\\n",
      "count                     15120.000000   15120.000000    15120.000000   \n",
      "mean                       1714.023214     212.704299      218.965608   \n",
      "std                        1325.066358      30.561287       22.801966   \n",
      "min                           0.000000       0.000000       99.000000   \n",
      "25%                         764.000000     196.000000      207.000000   \n",
      "50%                        1316.000000     220.000000      223.000000   \n",
      "75%                        2270.000000     235.000000      235.000000   \n",
      "max                        6890.000000     254.000000      254.000000   \n",
      "\n",
      "       Hillshade_3pm  Horizontal_Distance_To_Fire_Points  Wilderness_Area1  \\\n",
      "count   15120.000000                        15120.000000      15120.000000   \n",
      "mean      135.091997                         1511.147288          0.237897   \n",
      "std        45.895189                         1099.936493          0.425810   \n",
      "min         0.000000                            0.000000          0.000000   \n",
      "25%       106.000000                          730.000000          0.000000   \n",
      "50%       138.000000                         1256.000000          0.000000   \n",
      "75%       167.000000                         1988.250000          0.000000   \n",
      "max       248.000000                         6993.000000          1.000000   \n",
      "\n",
      "       Wilderness_Area2  Wilderness_Area3  Wilderness_Area4    Soil_Type1  \\\n",
      "count      15120.000000      15120.000000      15120.000000  15120.000000   \n",
      "mean           0.033003          0.419907          0.309193      0.023479   \n",
      "std            0.178649          0.493560          0.462176      0.151424   \n",
      "min            0.000000          0.000000          0.000000      0.000000   \n",
      "25%            0.000000          0.000000          0.000000      0.000000   \n",
      "50%            0.000000          0.000000          0.000000      0.000000   \n",
      "75%            0.000000          1.000000          1.000000      0.000000   \n",
      "max            1.000000          1.000000          1.000000      1.000000   \n",
      "\n",
      "         Soil_Type2    Soil_Type3    Soil_Type4    Soil_Type5    Soil_Type6  \\\n",
      "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
      "mean       0.041204      0.063624      0.055754      0.010913      0.042989   \n",
      "std        0.198768      0.244091      0.229454      0.103896      0.202840   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "       Soil_Type7    Soil_Type8    Soil_Type9   Soil_Type10   Soil_Type11  \\\n",
      "count     15120.0  15120.000000  15120.000000  15120.000000  15120.000000   \n",
      "mean          0.0      0.000066      0.000661      0.141667      0.026852   \n",
      "std           0.0      0.008133      0.025710      0.348719      0.161656   \n",
      "min           0.0      0.000000      0.000000      0.000000      0.000000   \n",
      "25%           0.0      0.000000      0.000000      0.000000      0.000000   \n",
      "50%           0.0      0.000000      0.000000      0.000000      0.000000   \n",
      "75%           0.0      0.000000      0.000000      0.000000      0.000000   \n",
      "max           0.0      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "        Soil_Type12   Soil_Type13   Soil_Type14  Soil_Type15   Soil_Type16  \\\n",
      "count  15120.000000  15120.000000  15120.000000      15120.0  15120.000000   \n",
      "mean       0.015013      0.031481      0.011177          0.0      0.007540   \n",
      "std        0.121609      0.174621      0.105133          0.0      0.086506   \n",
      "min        0.000000      0.000000      0.000000          0.0      0.000000   \n",
      "25%        0.000000      0.000000      0.000000          0.0      0.000000   \n",
      "50%        0.000000      0.000000      0.000000          0.0      0.000000   \n",
      "75%        0.000000      0.000000      0.000000          0.0      0.000000   \n",
      "max        1.000000      1.000000      1.000000          0.0      1.000000   \n",
      "\n",
      "        Soil_Type17   Soil_Type18   Soil_Type19   Soil_Type20   Soil_Type21  \\\n",
      "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
      "mean       0.040476      0.003968      0.003042      0.009193      0.001058   \n",
      "std        0.197080      0.062871      0.055075      0.095442      0.032514   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "        Soil_Type22   Soil_Type23   Soil_Type24   Soil_Type25   Soil_Type26  \\\n",
      "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
      "mean       0.022817      0.050066      0.016997      0.000066      0.003571   \n",
      "std        0.149326      0.218089      0.129265      0.008133      0.059657   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "        Soil_Type27   Soil_Type28   Soil_Type29   Soil_Type30   Soil_Type31  \\\n",
      "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
      "mean       0.000992      0.000595      0.085384      0.047950      0.021958   \n",
      "std        0.031482      0.024391      0.279461      0.213667      0.146550   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "        Soil_Type32   Soil_Type33   Soil_Type34   Soil_Type35   Soil_Type36  \\\n",
      "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
      "mean       0.045635      0.040741      0.001455      0.006746      0.000661   \n",
      "std        0.208699      0.197696      0.038118      0.081859      0.025710   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "        Soil_Type37   Soil_Type38   Soil_Type39   Soil_Type40    Cover_Type  \n",
      "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000  \n",
      "mean       0.002249      0.048148      0.043452      0.030357      4.000000  \n",
      "std        0.047368      0.214086      0.203880      0.171574      2.000066  \n",
      "min        0.000000      0.000000      0.000000      0.000000      1.000000  \n",
      "25%        0.000000      0.000000      0.000000      0.000000      2.000000  \n",
      "50%        0.000000      0.000000      0.000000      0.000000      4.000000  \n",
      "75%        0.000000      0.000000      0.000000      0.000000      6.000000  \n",
      "max        1.000000      1.000000      1.000000      1.000000      7.000000  \n"
     ]
    }
   ],
   "source": [
    "# Statistical description\n",
    "\n",
    "pandas.set_option('display.max_columns', None)\n",
    "print(dataset.describe())\n",
    "\n",
    "# Learning :\n",
    "# No attribute is missing as count is 15120 for all attributes. Hence, all rows can be used\n",
    "# Negative value(s) present in Vertical_Distance_To_Hydrology. Hence, some tests such as chi-sq cant be used.\n",
    "# Wilderness_Area and Soil_Type are one hot encoded. Hence, they could be converted back for some analysis\n",
    "# Attributes Soil_Type7 and Soil_Type15 can be removed as they are constant\n",
    "# Scales are not the same for all. Hence, rescaling and standardization may be necessary for some algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data statistics\n",
    "* Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Skewness of the distribution\n",
    "\n",
    "print(dataset.skew())\n",
    "\n",
    "# Values close to 0 show less skew\n",
    "# Several attributes in Soil_Type show a large skew. Hence, some algos may benefit if skew is corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data statistics\n",
    "* Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of instances belonging to each class\n",
    "\n",
    "dataset.groupby('Cover_Type').size()\n",
    "\n",
    "# We see that all classes have an equal presence. No class re-balancing is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Interaction\n",
    "* Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Correlation tells relation between two attributes.\n",
    "# Correlation requires continous data. Hence, ignore Wilderness_Area and Soil_Type as they are binary\n",
    "\n",
    "#sets the number of features considered\n",
    "size = 10 \n",
    "\n",
    "#create a dataframe with only 'size' features\n",
    "data=dataset.iloc[:,:size] \n",
    "\n",
    "#get the names of all the columns\n",
    "cols=data.columns \n",
    "\n",
    "# Calculates pearson co-efficient for all combinations\n",
    "data_corr = data.corr()\n",
    "\n",
    "# Set the threshold to select only only highly correlated attributes\n",
    "threshold = 0.5\n",
    "\n",
    "# List of pairs along with correlation above threshold\n",
    "corr_list = []\n",
    "\n",
    "#Search for the highly correlated pairs\n",
    "for i in range(0,size): #for 'size' features\n",
    "    for j in range(i+1,size): #avoid repetition\n",
    "        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\n",
    "            corr_list.append([data_corr.iloc[i,j],i,j]) #store correlation and columns index\n",
    "\n",
    "#Sort to show higher ones first            \n",
    "s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))\n",
    "\n",
    "#Print correlations and column names\n",
    "for v,i,j in s_corr_list:\n",
    "    print (\"%s and %s = %.2f\" % (cols[i],cols[j],v))\n",
    "\n",
    "# Strong correlation is observed between the following pairs\n",
    "# This represents an opportunity to reduce the feature set through transformations such as PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Interaction\n",
    "* Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of only the highly correlated pairs\n",
    "for v,i,j in s_corr_list:\n",
    "    sns.pairplot(dataset, hue=\"Cover_Type\", size=6, x_vars=cols[i],y_vars=cols[j] )\n",
    "    plt.show()\n",
    "\n",
    "#The plots show to which class does a point belong to. The class distribution overlaps in the plots.    \n",
    "#Hillshade patterns give a nice ellipsoid patterns with each other\n",
    "#Aspect and Hillshades attributes form a sigmoid pattern\n",
    "#Horizontal and vertical distance to hydrology give an almost linear pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "* Box and density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will visualize all the attributes using Violin Plot - a combination of box and density plots\n",
    "\n",
    "#names of all the attributes \n",
    "cols = dataset.columns\n",
    "\n",
    "#number of attributes (exclude target)\n",
    "size = len(cols)-1\n",
    "\n",
    "#x-axis has target attribute to distinguish between classes\n",
    "x = cols[size]\n",
    "\n",
    "#y-axis shows values of an attribute\n",
    "y = cols[0:size]\n",
    "\n",
    "#Plot violin for all attributes\n",
    "for i in range(0,size):\n",
    "    sns.violinplot(data=dataset,x=x,y=y[i])  \n",
    "    plt.show()\n",
    "\n",
    "#Elevation is has a separate distribution for most classes. Highly correlated with the target and hence an important attribute\n",
    "#Aspect contains a couple of normal distribution for several classes\n",
    "#Horizontal distance to road and hydrology have similar distribution\n",
    "#Hillshade 9am and 12pm display left skew\n",
    "#Hillshade 3pm is normal\n",
    "#Lots of 0s in vertical distance to hydrology\n",
    "#Wilderness_Area3 gives no class distinction. As values are not present, others gives some scope to distinguish\n",
    "#Soil_Type, 1,5,8,9,12,14,18-22, 25-30 and 35-40 offer class distinction as values are not present for many classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "* Grouping of One hot encoded attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group one-hot encoded variables of a category into one single variable\n",
    "\n",
    "#names of all the columns\n",
    "cols = dataset.columns\n",
    "\n",
    "#number of rows=r , number of columns=c\n",
    "r,c = dataset.shape\n",
    "\n",
    "#Create a new dataframe with r rows, one column for each encoded category, and target in the end\n",
    "data = pandas.DataFrame(index=numpy.arange(0, r),columns=['Wilderness_Area','Soil_Type','Cover_Type'])\n",
    "\n",
    "#Make an entry in 'data' for each r as category_id, target value\n",
    "for i in range(0,r):\n",
    "    w=0;\n",
    "    s=0;\n",
    "    # Category1 range\n",
    "    for j in range(10,14):\n",
    "        if (dataset.iloc[i,j] == 1):\n",
    "            w=j-9  #category class\n",
    "            break\n",
    "    # Category2 range        \n",
    "    for k in range(14,54):\n",
    "        if (dataset.iloc[i,k] == 1):\n",
    "            s=k-13 #category class\n",
    "            break\n",
    "    #Make an entry in 'data' for each r as category_id, target value        \n",
    "    data.iloc[i]=[w,s,dataset.iloc[i,c-1]]\n",
    "\n",
    "#Plot for Category1    \n",
    "sns.countplot(x=\"Wilderness_Area\", hue=\"Cover_Type\", data=data)\n",
    "plt.show()\n",
    "#Plot for Category2\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "sns.countplot(x=\"Soil_Type\", hue=\"Cover_Type\", data=data)\n",
    "plt.show()\n",
    "\n",
    "#(right-click and open the image in a new window for larger size)\n",
    "#WildernessArea_4 has a lot of presence for cover_type 4. Good class distinction\n",
    "#WildernessArea_3 has not much class distinction\n",
    "#SoilType 1-6,10-14,17, 22-23, 29-33,35,38-40 offer lot of class distinction as counts for some are very high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "* Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removal list initialize\n",
    "rem = []\n",
    "\n",
    "#Add constant columns as they don't help in prediction process\n",
    "for c in dataset.columns:\n",
    "    if dataset[c].std() == 0: #standard deviation is zero\n",
    "        rem.append(c)\n",
    "\n",
    "#drop the columns        \n",
    "dataset.drop(rem,axis=1,inplace=True)\n",
    "\n",
    "print(rem)\n",
    "\n",
    "#Following columns are dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "* Original\n",
    "* Delete rows or impute values in case of missing\n",
    "* StandardScaler\n",
    "* MinMaxScaler\n",
    "* Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the number of rows and columns\n",
    "r, c = dataset.shape\n",
    "\n",
    "#get the list of columns\n",
    "cols = dataset.columns\n",
    "#create an array which has indexes of columns\n",
    "i_cols = []\n",
    "for i in range(0,c-1):\n",
    "    i_cols.append(i)\n",
    "#array of importance rank of all features  \n",
    "ranks = []\n",
    "\n",
    "#Extract only the values\n",
    "array = dataset.values\n",
    "\n",
    "#Y is the target column, X has the rest\n",
    "X = array[:,0:(c-1)]\n",
    "Y = array[:,(c-1)]\n",
    "\n",
    "#Validation chunk size\n",
    "val_size = 0.1\n",
    "\n",
    "#Use a common seed in all experiments so that same chunk is used for validation\n",
    "seed = 0\n",
    "\n",
    "#Split the data into chunks\n",
    "from sklearn import cross_validation\n",
    "X_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed)\n",
    "\n",
    "#Import libraries for data transformations\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#All features\n",
    "X_all = []\n",
    "#Additionally we will make a list of subsets\n",
    "X_all_add =[]\n",
    "\n",
    "#columns to be dropped\n",
    "rem = []\n",
    "#indexes of columns to be dropped\n",
    "i_rem = []\n",
    "\n",
    "#List of combinations\n",
    "comb = []\n",
    "comb.append(\"All+1.0\")\n",
    "\n",
    "#Add this version of X to the list \n",
    "X_all.append(['Orig','All', X_train,X_val,1.0,cols[:c-1],rem,ranks,i_cols,i_rem])\n",
    "\n",
    "#point where categorical data begins\n",
    "size=10\n",
    "\n",
    "#Standardized\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = StandardScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['StdSca','All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n",
    "\n",
    "#MinMax\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['MinMax', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n",
    "\n",
    "#Normalize\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = Normalizer().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['Norm', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n",
    "\n",
    "#Impute\n",
    "#Imputer is not used as no data is missing\n",
    "\n",
    "#List of transformations\n",
    "trans_list = []\n",
    "\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "    trans_list.append(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "* ExtraTreesClassifier\n",
    "* GradientBoostingClassifier\n",
    "* RandomForestClassifier\n",
    "* XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select top 75%,50%,25%\n",
    "ratio_list = [0.75,0.50,0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of feature selection models\n",
    "feat = []\n",
    "\n",
    "#List of names of feature selection models\n",
    "feat_list =[]\n",
    "\n",
    "#Import the libraries\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Add ExtraTreeClassifiers to the list\n",
    "n = 'ExTree'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,ExtraTreesClassifier(n_estimators=c-1,max_features=val,n_jobs=-1,random_state=seed)])      \n",
    "\n",
    "#Add GradientBoostingClassifiers to the list \n",
    "n = 'GraBst'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,GradientBoostingClassifier(n_estimators=c-1,max_features=val,random_state=seed)])   \n",
    "\n",
    "#Add RandomForestClassifiers to the list \n",
    "n = 'RndFst'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,RandomForestClassifier(n_estimators=c-1,max_features=val,n_jobs=-1,random_state=seed)])   \n",
    "\n",
    "#Add XGBClassifier to the list \n",
    "n = 'XGB'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,XGBClassifier(n_estimators=c-1,seed=seed)])   \n",
    "        \n",
    "#For all transformations of X\n",
    "for trans,s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n",
    "    #For all feature selection models\n",
    "    for name,v, model in feat:\n",
    "        #Train the model against Y\n",
    "        model.fit(X,Y_train)\n",
    "        #Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        for i, pred in enumerate(list(model.feature_importances_)):\n",
    "            joined.append([i,cols[i],pred])\n",
    "        #Sort in descending order    \n",
    "        joined_sorted = sorted(joined, key=lambda x: -x[2])\n",
    "        #Starting point of the columns to be dropped\n",
    "        rem_start = int((v*(c-1)))\n",
    "        #List of names of columns selected\n",
    "        cols_list = []\n",
    "        #Indexes of columns selected\n",
    "        i_cols_list = []\n",
    "        #Ranking of all the columns\n",
    "        rank_list =[]\n",
    "        #List of columns not selected\n",
    "        rem_list = []\n",
    "        #Indexes of columns not selected\n",
    "        i_rem_list = []\n",
    "        #Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            #Store the rank\n",
    "            rank_list.append([i,j])\n",
    "            #Store selected columns in cols_list and indexes in i_cols_list\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            #Store not selected columns in rem_list and indexes in i_rem_list    \n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)    \n",
    "        #Sort the rank_list and store only the ranks. Drop the index \n",
    "        #Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])    \n",
    "\n",
    "#Set figure size\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Plot a graph for different feature selectors        \n",
    "for f_name in feat_list:\n",
    "    #Array to store the list of combinations\n",
    "    leg=[]\n",
    "    fig, ax = plt.subplots()\n",
    "    #Plot each combination\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        if(name==f_name):\n",
    "            plt.plot(rank_list)\n",
    "            leg.append(trans+\"+\"+name+\"+%s\"% v)\n",
    "    #Set the tick names to names of columns\n",
    "    ax.set_xticks(range(c-1))\n",
    "    ax.set_xticklabels(cols[:c-1],rotation='vertical')\n",
    "    #Display the plot\n",
    "    plt.legend(leg,loc='best')    \n",
    "    #Plot the rankings of all the features for all combinations\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "* RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of feature selection models\n",
    "feat = []\n",
    "\n",
    "#List of names of feature selection models\n",
    "feat_list =[]\n",
    "\n",
    "#Libraries for feature selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Add RFE to the list \n",
    "model = LogisticRegression(random_state=seed,n_jobs=-1)\n",
    "n = 'RFE'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,RFE(model,val*(c-1))])   \n",
    "        \n",
    "#For all transformations of X\n",
    "for trans,s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n",
    "    #For all feature selection models\n",
    "    for name,v, model in feat:\n",
    "        #Train the model against Y\n",
    "        model.fit(X,Y_train)\n",
    "        #Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        for i, pred in enumerate(list(model.ranking_)):\n",
    "            joined.append([i,cols[i],pred])\n",
    "        #Sort in ascending order    \n",
    "        joined_sorted = sorted(joined, key=lambda x: x[2])\n",
    "        #Starting point of the columns to be dropped\n",
    "        rem_start = int((v*(c-1)))\n",
    "        #List of names of columns selected\n",
    "        cols_list = []\n",
    "        #Indexes of columns selected\n",
    "        i_cols_list = []\n",
    "        #Ranking of all the columns\n",
    "        rank_list =[]\n",
    "        #List of columns not selected\n",
    "        rem_list = []\n",
    "        #Indexes of columns not selected\n",
    "        i_rem_list = []\n",
    "        #Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for i, col, j in joined_sorted:\n",
    "            #Store the rank\n",
    "            rank_list.append([i,j-1])\n",
    "            #Store selected columns in cols_list and indexes in i_cols_list\n",
    "            if((j-1) < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            #Store not selected columns in rem_list and indexes in i_rem_list    \n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)    \n",
    "        #Sort the rank_list and store only the ranks. Drop the index \n",
    "        #Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])    \n",
    "\n",
    "#Set figure size\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Plot a graph for different feature selectors        \n",
    "for f_name in feat_list:\n",
    "    #Array to store the list of combinations\n",
    "    leg=[]\n",
    "    fig, ax = plt.subplots()\n",
    "    #Plot each combination\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        if(name==f_name):\n",
    "            plt.plot(rank_list)\n",
    "            leg.append(trans+\"+\"+name+\"+%s\"% v)\n",
    "    #Set the tick names to names of columns\n",
    "    ax.set_xticks(range(c-1))\n",
    "    ax.set_xticklabels(cols[:c-1],rotation='vertical')\n",
    "    #Display the plot\n",
    "    plt.legend(leg,loc='best')    \n",
    "    #Plot the rankings of all the features for all combinations\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Selection\n",
    "* SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of feature selection models\n",
    "feat = []\n",
    "\n",
    "#List of names of feature selection models\n",
    "feat_list =[]\n",
    "\n",
    "#Libraries for SelectPercentile    \n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif        \n",
    "\n",
    "n = 'SelK'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,SelectPercentile(score_func=f_classif,percentile=val*100)])   \n",
    "\n",
    "#For all transformations of X\n",
    "for trans,s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n",
    "    #For all feature selection models\n",
    "    for name,v, model in feat:\n",
    "        #Train the model against Y\n",
    "        model.fit(X,Y_train)\n",
    "        #Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        for i, pred in enumerate(list(model.scores_)):\n",
    "            joined.append([i,cols[i],pred])\n",
    "        #Sort in descending order    \n",
    "        joined_sorted = sorted(joined, key=lambda x: -x[2])\n",
    "        #Starting point of the columns to be dropped\n",
    "        rem_start = int((v*(c-1)))\n",
    "        #List of names of columns selected\n",
    "        cols_list = []\n",
    "        #Indexes of columns selected\n",
    "        i_cols_list = []\n",
    "        #Ranking of all the columns\n",
    "        rank_list =[]\n",
    "        #List of columns not selected\n",
    "        rem_list = []\n",
    "        #Indexes of columns not selected\n",
    "        i_rem_list = []\n",
    "        #Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            #Store the rank\n",
    "            rank_list.append([i,j])\n",
    "            #Store selected columns in cols_list and indexes in i_cols_list\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            #Store not selected columns in rem_list and indexes in i_rem_list    \n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)    \n",
    "        #Sort the rank_list and store only the ranks. Drop the index \n",
    "        #Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])    \n",
    "\n",
    "\n",
    "#Set figure size\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Plot a graph for different feature selectors        \n",
    "for f_name in feat_list:\n",
    "    #Array to store the list of combinations\n",
    "    leg=[]\n",
    "    fig, ax = plt.subplots()\n",
    "    #Plot each combination\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        if(name==f_name):\n",
    "            plt.plot(rank_list)\n",
    "            leg.append(trans+\"+\"+name+\"+%s\"% v)\n",
    "    #Set the tick names to names of columns\n",
    "    ax.set_xticks(range(c-1))\n",
    "    ax.set_xticklabels(cols[:c-1],rotation='vertical')\n",
    "    #Display the plot\n",
    "    plt.legend(leg,loc='best')    \n",
    "    #Plot the rankings of all the features for all combinations\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Selection\n",
    "Ranking summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank_df = pandas.DataFrame(data=[x[7] for x in X_all_add],columns=cols[:c-1])\n",
    "_ = rank_df.boxplot(rot=90)\n",
    "#Below plot summarizes the rankings according to the standard feature selection techniques\n",
    "#Top ranked attributes are ... first 10 attributes, Wilderness_Area1,4 ...Soil_Type 3,4,10,38-40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Selection\n",
    "Rank features based on median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank_df = pandas.DataFrame(data=[x[7] for x in X_all_add],columns=cols[:c-1])\n",
    "med = rank_df.median()\n",
    "print(med)\n",
    "#Write medians to output file for exploratory study on ML algorithms\n",
    "with open(\"median.csv\", \"w\") as subfile:\n",
    "       subfile.write(\"Column,Median\\n\")\n",
    "       subfile.write(med.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 2 of the Notebook:\n",
    "https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-of-ml-algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
